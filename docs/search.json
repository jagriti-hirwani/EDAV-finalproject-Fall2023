[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "USA Layoff Analysis",
    "section": "",
    "text": "1 Introduction\nLayoffs can have significant and widespread effects on individuals, communities, and the economy as a whole. This is particularly pertinent for current students who are navigating the job market and assessing future career prospects. Our goal is to unveil the nuanced reality behind layoffs, posing essential questions to empower individuals in their career planning, assist businesses in shaping effective strategies, and guide policymakers in addressing challenges tied to workforce transitions and economic uncertainties.\nKey questions we aim to address include:\n\nIndustry Resilience\n\n\nWhich industries or sectors are experiencing the highest levels of layoffs? Which industries are more affected by layoffs and which ones are more robust to the economic conditions?\n\n\nGeographic Dynamics\n\n\nHow do layoff rates vary across different geographic regions?\nCan we discern disparities in layoff rates between urban and rural areas?\n\n\nEconomic and Market Correlation\n\n\nHow do layoff rates correlate with market and economic indicators?\nIs the unemployment rate also consistent with the observed layoff data?\n\n\nTemporal Trends\n\n\nWhat is the historical trend of layoffs across different years?\nAre there discernible patterns or cycles in layoff data, and how closely do these align with broader market trends and stock market data?\n\n\nNotice period\n\n\nWhat is the typical notice period provided to employees facing layoffs?\nAre employees awarded sufficient time to explore alternative opportunities, and how can this impact their transition?\n\n\nReason for Layoffs\n\n\nWhat was the major reason for layoffs? Which reasons were more common than the others?\n\nBy addressing these questions, our analysis seeks to provide a comprehensive understanding of the layoff landscape, offering insights that can inform proactive career choices, strategic business decisions, and effective policy interventions. This knowledge is crucial for navigating the complexities of a dynamic job market and fostering resilience in the face of economic changes."
  },
  {
    "objectID": "data.html#technical-description",
    "href": "data.html#technical-description",
    "title": "2  Data",
    "section": "2.1 Technical Description",
    "text": "2.1 Technical Description\nThe datasets we are using for our project are as follows:\n\nLayoff Data: https://layoffdata.com/data/Raw/Stock_Indices/\n\n\nCollected by: The federal WARN Act requires large employers to give advance notice of layoffs to state governments and workers. Though states publish this information, no entity collects these layoff notices across the many states. The WARN Database standardizes WARN Notices across the country into a single dataset and the only comprehensive database of worker layoffs.\nDownloaded as an xlsx file from the website\nFormat of the data: excel spreadsheet with the following features:\n\nState\nCompany\nCity\nNumber of Workers laid off\nReceived Date - date when the notice was received\nEffective Date - date of leaving\nLayoff/Closure Type - reason for layoff\nUnion - union to which workers belonged to if any\nRegion\nCounty\nIndustry - type of industry that the company belongs\nNotes - further information on the reason for layoff\nFrequency of updates: Monthly\n\nDimensions of the data: The data consists of 50218 rows and 13 columns.\nIssues/problems with the data: There are two columns in the data namely: ‘Layoff/ClosureType’ and ‘Temporary/Permanent’ which have coinciding values of the reason for layoff. We will pick one which is more explanatory of the layoff cause.\n\n\nUnemployment Rate Data: https://fred.stlouisfed.org/series/UNRATE\n\n\nCollected by Federal Reserve Economic Data from the U.S. Bureau of Labor Statistics\nDownload as an excel file available on the website\nFormat of the data : .xls file with the following features\n\nObservation date: date on which the unemployment rate was taken\nRate of unemployment\n\nFrequency of update: Monthly\nDimensions of the data: the data has 910 rows and 2 columns\nOther relevant information: the data consists monthly employment rate from January 1984 - October 2023\nIssues/problems with the data: there is no problem with the data, we will trim the data to accommodate recent years as we do not need unemployment rates as old as 1984.\n\n\nStock market data: https://www.wsj.com/market-data/quotes/index/SPX/historical-prices\n\n\nCollected by the Wall Street Journal\nDownloaded as a csv file\nFormat of the data: .csv file with the following features:\n\nDate\nOpen - opening prices for the day\nHigh - highest price of the day\nLow - lowest price of the day\n\nClose - closing prices for the day\nFrequency of the data: Daily\nDimensions of the data: The data has 4089 rows and 5 columns\nOther relevant information: The data ranges from September 2007 to November 2023\n\nPlan to import the data: We have downloaded the data from the sources mentioned above and will be using it in our R workspace for further analysis."
  },
  {
    "objectID": "data.html#research-plan",
    "href": "data.html#research-plan",
    "title": "2  Data",
    "section": "2.2 Research Plan",
    "text": "2.2 Research Plan\nCommencing our analysis, we will direct our attention to current layoff statistics over time, with the aim of understanding prevailing trends in the job market landscape. If current layoff data shows a notable increase, we can reasonably infer a potentially challenging job market scenario.\nSimultaneously, we will conduct a comparative analysis with key US stock indices, particularly the S&P 500, to identify potential correlations with market performance. We anticipate a negative correlation, suggesting that a sluggish market performance may coincide with an uptick in current job terminations as organizations strive to optimize expenditures amidst reduced business activity.\nDeepening our inquiry, our objective will be to identify patterns that could serve as indicators for current trajectories. Leveraging current layoff data, we will systematically identify sectors experiencing higher instances of workforce reduction. With a focus on the period from 2018 to 2023, encompassing the last five years, we acknowledge the challenge of comparing raw layoff numbers across industries due to variations in total employment. Therefore, our approach will involve an intra-industry comparison over time, aiming to distill trends that may identify the most impacted sectors. Conversely, industries with a relative absence of pronounced spikes in current layoffs may be considered more resilient in economic downturns.\nExpanding our analysis, we will shift focus to a geographically nuanced examination of trends within the current US landscape. This aims to determine whether specific states or regions bear a disproportionate burden of current layoffs. Integrating current regional data with industry-specific metrics will provide insights into whether certain industries within particular states face more challenging economic circumstances.\nContinuing our trajectory, we will examine the current temporal dynamics of layoffs, looking at the pace at which organizations are implementing workforce reductions. Analyzing the time organizations are affording employees to adapt to impending layoffs across diverse industries, we will also consider state-by-state variations to understand the influence of current regional legislative frameworks.\nExamining the causative factors behind current layoffs available in data, we will undertake a meticulous analysis to identify predominant factors shaping current workforce reduction scenarios. This investigative phase, adopting a region-wise and state-wise lens, will aim to discern nuanced patterns that may emerge.\nFinally, our analysis will integrate unemployment rate data to understand the post-layoff trajectories of affected individuals. This examination seeks to ascertain whether individuals subjected to current layoffs will transition swiftly into alternative employment or contribute to an uptick in current unemployment rates by remaining unemployed."
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.3 Missing value analysis",
    "text": "2.3 Missing value analysis\nWe will now investigate our data to check for any missing values.\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(readxl)\n\n\n\n2.3.1 Layoff Dataset\n\n\nCode\ndata &lt;- read_excel(\"./data/Raw/Layoff/WARN Database 10-31-2023 [TO EDIT_ FILE-_MAKE A COPY].xlsx\")\n\nhead(data, 5)\n\n\n# A tibble: 5 × 13\n  State Company City  `Number of Workers` `Received Date`     `Effective\\n Date`\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;               &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;             \n1 Utah  SKF     Salt…                 108 2023-12-04 00:00:00 &lt;NA&gt;              \n2 Utah  Walgre… Salt…                  70 2023-11-29 00:00:00 &lt;NA&gt;              \n3 Alab… Thermo… Aubu…                  97 2023-11-03 00:00:00 45293.0           \n4 Mass… Omnica… Peab…                  53 2023-11-03 00:00:00 45298.0           \n5 Geor… All FA… Fair…                  50 2023-11-02 00:00:00 45291.0           \n# ℹ 7 more variables: `Layoff/Closure\\n Type` &lt;chr&gt;,\n#   `Temporary/Permanent` &lt;chr&gt;, Union &lt;chr&gt;, Region &lt;chr&gt;, County &lt;chr&gt;,\n#   Industry &lt;chr&gt;, Notes &lt;chr&gt;\n\n\nLet’s check for the distribution of missing values across columns in our data.\n\n\nCode\n# Calculate the number of missing values in each column\nmissing_values &lt;- sapply(data, function(x) sum(is.na(x)))\n\n# Calculate the total number of records in the dataset\ntotal_records &lt;- nrow(data)\n\n# Create a data frame for plotting\nmissing_data_df &lt;- data.frame(Column = names(missing_values), Missing_Values = missing_values)\n\n# Create a bar chart using ggplot2 with text labels on top and total records\nggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +\n    geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n    geom_text(aes(label = Missing_Values), vjust = -0.5, color = \"black\") +\n    geom_text(aes(x = 0, y = total_records + 5, label = paste(\"Total Records:\", total_records)),\n        hjust = 0, vjust = 0, color = \"black\"\n    ) +\n    labs(\n        title = \"Number of Missing Values in Each Column\",\n        x = \"Column\",\n        y = \"Number of Missing Values\"\n    ) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWe can make the following observations:\n\nWe have Company and State information for all records.\n‘Union’ and ‘Temporary/Permanent’ information is missing for almost all records, so we cannot include them in our analysis.\nWe have around 612 records with no information of the ‘Number of Workers’ laid off, so these records cannot contribute to our analysis.\nColumns like ‘Notes’, and ‘Region’ have high fraction of missing values but they don’t play a major role in our analysis.\nThe high fraction of missing values in ‘Industry’ coulmn, however, could limit our industry wise analysis. Since the critical parts of our analysis will depend on the last 5 years, we can check the missing ratio in that time interval to check if it’s any better.\n\n\n\nCode\ndata$`Received Date` &lt;- as.POSIXct(data$`Received Date`, format = \"%Y-%m-%d %Z\")\n\n# Filter for values after 2018\nfiltered_data &lt;- data %&gt;%\n    filter(`Received Date` &gt; as.POSIXct(\"2018-01-01\"))\n\n\n# Calculate the number of missing values in each column\nmissing_values &lt;- sapply(filtered_data, function(x) sum(is.na(x)))\n\n# Calculate the total number of records in the dataset\ntotal_records &lt;- nrow(filtered_data)\n\n# Create a data frame for plotting\nmissing_data_df &lt;- data.frame(Column = names(missing_values), Missing_Values = missing_values)\n\n# Create a bar chart using ggplot2 with text labels on top and total records\nggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +\n    geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n    geom_text(aes(label = Missing_Values), vjust = -0.5, color = \"black\") +\n    geom_text(aes(x = 0, y = total_records + 5, label = paste(\"Total Records:\", total_records)),\n        hjust = 0, vjust = 0, color = \"black\"\n    ) +\n    labs(\n        title = \"Number of Missing Values in Each Column (2018-2023)\",\n        x = \"Column\",\n        y = \"Number of Missing Values\"\n    ) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWe do not see any good signs in this time interval since the fraction of missing values in the ‘Industry’ column is still high. Although, we see that we don’t have any missing values in the ‘Number of Workers’ laid off.\nNow, switching back to our original dataset, let’s drop the ‘Union’, ‘Temporary/Permanent’, ‘Notes’, ‘Region’ columns before we deep dive into our missing values analyis as they might shadow the otherwise interesting patterns. We will also drop records with missing values in the ‘Number of Workers’ laid off.\n\n\nCode\n# Clean the data\ndata &lt;- data |&gt;\n    select(-\"Union\", -\"Temporary/Permanent\", -\"Notes\", -\"Region\")\n\ndata &lt;- data[!is.na(data[, \"Number of Workers\"]), ]\nhead(data)\n\n\n# A tibble: 6 × 9\n  State Company City  `Number of Workers` `Received Date`     `Effective\\n Date`\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;               &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;             \n1 Utah  SKF     Salt…                 108 2023-12-04 00:00:00 &lt;NA&gt;              \n2 Utah  Walgre… Salt…                  70 2023-11-29 00:00:00 &lt;NA&gt;              \n3 Alab… Thermo… Aubu…                  97 2023-11-03 00:00:00 45293.0           \n4 Mass… Omnica… Peab…                  53 2023-11-03 00:00:00 45298.0           \n5 Geor… All FA… Fair…                  50 2023-11-02 00:00:00 45291.0           \n6 Nebr… Bank o… Omaha                  41 2023-11-02 00:00:00 45317.0           \n# ℹ 3 more variables: `Layoff/Closure\\n Type` &lt;chr&gt;, County &lt;chr&gt;,\n#   Industry &lt;chr&gt;\n\n\nNow, to identify patterns in missing values, we added a new feature to the redav package to address the chalenges of large datasets. We added the parameter ‘threshold’. This controls the percentage of records covered by patterns to be displayed in the graph. For example, if we want to just display patterns which cover 90% of our data, we can pass a value of 0.9.\nAdditionally, we also inclined the x-labels.\n\n\nCode\nplot_missing &lt;- function(x, percent = TRUE, threshold = 1) {\n    na_count_all &lt;- data.frame(is.na(x)) %&gt;%\n        dplyr::group_by_all() %&gt;%\n        dplyr::count(name = \"count\", sort = TRUE) %&gt;%\n        dplyr::ungroup() %&gt;%\n        tibble::rownames_to_column(\"pattern\")\n\n    na_count_all &lt;- na_count_all %&gt;%\n        dplyr::mutate(pattern = factor(.data$pattern, levels = nrow(na_count_all):1))\n\n    # count the number of columns with missing values; will be used later to determine if there's a \"none missing\" pattern\n    na_count_all &lt;- na_count_all %&gt;%\n        dplyr::rowwise() %&gt;%\n        dplyr::mutate(num_missing_cols = sum(dplyr::c_across(where(is.logical))))\n\n    # filter out based on threshold : NEWLY ADDED\n    na_count_all &lt;- na_count_all %&gt;%\n        arrange(desc(count)) %&gt;%\n        mutate(perc_pattern_cov = count / sum(na_count_all$count, na.rm = TRUE))\n    na_count_all$cum_perc_pattern &lt;- cumsum(na_count_all$perc_pattern_cov)\n    na_count_all &lt;- na_count_all |&gt;\n        filter(cum_perc_pattern &lt;= threshold) |&gt;\n        select(-cum_perc_pattern, -perc_pattern_cov)\n\n    # data frame for missing patterns bar chart\n    na_count_by_pattern &lt;- na_count_all[, c(\"pattern\", \"count\", \"num_missing_cols\")]\n    na_count_by_pattern$none_missing &lt;- ifelse(na_count_by_pattern$num_missing_cols == 0, TRUE, FALSE)\n\n    # data frame for missing by column bar chart\n    na_count_by_column &lt;- data.frame(is.na(x)) %&gt;%\n        colSums() %&gt;%\n        sort(decreasing = TRUE) %&gt;%\n        tibble::enframe(name = \"var\", value = \"count\")\n\n    # tidy and sort na_count_all by column counts\n    na_count_all_tidy &lt;- na_count_all %&gt;%\n        tidyr::pivot_longer(where(is.logical), names_to = \"variable\") %&gt;%\n        dplyr::mutate(variable = factor(.data$variable, levels = na_count_by_column$var)) %&gt;%\n        dplyr::mutate(none_missing = ifelse(.data$num_missing_cols == 0, TRUE, FALSE))\n\n    # main plot\n    main_plot &lt;- ggplot2::ggplot(na_count_all_tidy, ggplot2::aes(.data$variable, .data$pattern, fill = factor(.data$value), alpha = .data$none_missing)) +\n        ggplot2::geom_tile(color = \"white\") +\n        ggplot2::scale_fill_manual(values = c(\"grey70\", \"mediumpurple\")) +\n        ggplot2::scale_alpha_manual(values = c(.7, 1)) +\n        ggplot2::ylab(\"missing pattern\") +\n        ggplot2::guides(fill = \"none\", alpha = \"none\") +\n        ggplot2::theme_classic(12) +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1)) #NEWLY ADDED\n\n    # check for \"none missing\" pattern\n    none_missing_pattern &lt;- na_count_by_pattern %&gt;%\n        dplyr::filter(.data$none_missing) %&gt;%\n        dplyr::pull(.data$pattern)\n\n    if (length(none_missing_pattern) &gt; 0) {\n        main_plot &lt;- main_plot +\n            ggplot2::annotate(\"text\",\n                x = (ncol(na_count_all) - 2) / 2,\n                y = nrow(na_count_all) + 1 - as.numeric(as.character(none_missing_pattern)),\n                label = \"complete cases\"\n            )\n    }\n\n    # margin plots\n\n    denom &lt;- ifelse(percent, nrow(x) / 100, 1)\n\n    missing_by_column_plot &lt;- ggplot2::ggplot(na_count_by_column, ggplot2::aes(forcats::fct_inorder(.data$var), .data$count / denom)) +\n        ggplot2::geom_col(fill = \"cornflowerblue\", alpha = .7) +\n        ggplot2::scale_y_continuous(expand = c(0, 0), n.breaks = 3) +\n        ggplot2::xlab(\"\") +\n        ggplot2::ylab(ifelse(percent, \"% rows \\n missing:\", \"num rows \\n missing:\")) +\n        ggplot2::theme_linedraw(12) +\n        ggplot2::theme(\n            panel.grid.major.x = ggplot2::element_blank(),\n            panel.grid.minor.x = ggplot2::element_blank()\n        ) +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1)) # NEWLY ADDED\n\n    missing_by_pattern_plot &lt;-\n        ggplot2::ggplot(na_count_by_pattern, ggplot2::aes(.data$pattern, .data$count / denom, alpha = .data$none_missing)) +\n        ggplot2::geom_col(fill = \"cornflowerblue\") +\n        ggplot2::coord_flip() +\n        ggplot2::scale_y_continuous(expand = c(0, 0), n.breaks = 3) +\n        ggplot2::scale_alpha_manual(values = c(.7, 1)) +\n        ggplot2::xlab(\"\") +\n        ggplot2::ylab(ifelse(percent, \"% rows\", \"row count\")) +\n        ggplot2::guides(alpha = \"none\") +\n        ggplot2::theme_linedraw(12) +\n        ggplot2::theme(\n            panel.grid.major.y = ggplot2::element_blank(),\n            panel.grid.minor.y = ggplot2::element_blank()\n        )\n\n    if (percent) {\n        missing_by_column_plot &lt;- missing_by_column_plot +\n            ggplot2::scale_y_continuous(\n                expand = c(0, 0), n.breaks = 5,\n                limits = c(0, 100)\n            )\n        missing_by_pattern_plot &lt;- missing_by_pattern_plot +\n            ggplot2::scale_y_continuous(\n                expand = c(0, 0), n.breaks = 5,\n                limits = c(0, 100)\n            )\n    }\n\n    missing_by_column_plot + patchwork::plot_spacer() +\n        main_plot + missing_by_pattern_plot +\n        patchwork::plot_layout(widths = c(4, 1), heights = c(1, 4))\n}\n\n\n\n\nCode\nplot_missing(data, threshold = 0.99, percent = FALSE)\n\n\n\n\n\nWe can make the following observations:\n\nAs expected, ‘Industry’ column accounts for most missing values but in most cases it is the only column absent.\n‘Complete Cases’ form the second most observed pattern which is a good sign.\n\n\n\n2.3.2 Unemployment Dataset\n\n\nCode\ndata &lt;- read_excel(\"./data/Raw/Unemployment/UNRATE.xls\", skip=10)\n\nhead(data, 5)\n\n\n# A tibble: 5 × 2\n  observation_date    UNRATE\n  &lt;dttm&gt;               &lt;dbl&gt;\n1 1948-01-01 00:00:00    3.4\n2 1948-02-01 00:00:00    3.8\n3 1948-03-01 00:00:00    4  \n4 1948-04-01 00:00:00    3.9\n5 1948-05-01 00:00:00    3.5\n\n\nLet’s check for the distribution of missing values across columns in our data.\n\n\nCode\n# Calculate the number of missing values in each column\nmissing_values &lt;- sapply(data, function(x) sum(is.na(x)))\n\n# Calculate the total number of records in the dataset\ntotal_records &lt;- nrow(data)\n\n# Create a data frame for plotting\nmissing_data_df &lt;- data.frame(Column = names(missing_values), Missing_Values = missing_values)\n\n# Create a bar chart using ggplot2 with text labels on top and total records\nggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +\n    geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n    geom_text(aes(label = Missing_Values), vjust = -0.5, color = \"black\") +\n    geom_text(aes(x = 0, y = total_records + 5, label = paste(\"Total Records:\", total_records)),\n        hjust = 0, vjust = 0, color = \"black\"\n    ) +\n    labs(\n        title = \"Number of Missing Values in Each Column\",\n        x = \"Column\",\n        y = \"Number of Missing Values\"\n    ) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWe can see that we have no missing values.\n\n\n2.3.3 Stock Market Dataset (S&P500)\n\n\nCode\ndata &lt;- read.csv(\"./data/Raw/Stock_Indices/S&P500.csv\")\n\nhead(data, 5)\n\n\n      Date    Open    High     Low   Close\n1 11/29/23 4571.84 4587.64 4547.15 4550.58\n2 11/28/23 4545.55 4568.14 4540.51 4554.89\n3 11/27/23 4554.86 4560.52 4546.32 4550.43\n4 11/24/23 4555.84 4560.31 4552.80 4559.34\n5 11/22/23 4553.04 4568.43 4545.05 4556.62\n\n\nLet’s check for the distribution of missing values across columns in our data.\n\n\nCode\n# Calculate the number of missing values in each column\nmissing_values &lt;- sapply(data, function(x) sum(is.na(x)))\n\n# Calculate the total number of records in the dataset\ntotal_records &lt;- nrow(data)\n\n# Create a data frame for plotting\nmissing_data_df &lt;- data.frame(Column = names(missing_values), Missing_Values = missing_values)\n\n# Create a bar chart using ggplot2 with text labels on top and total records\nggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +\n    geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n    geom_text(aes(label = Missing_Values), vjust = -0.5, color = \"black\") +\n    geom_text(aes(x = 0, y = total_records + 5, label = paste(\"Total Records:\", total_records)),\n        hjust = 0, vjust = 0, color = \"black\"\n    ) +\n    labs(\n        title = \"Number of Missing Values in Each Column\",\n        x = \"Column\",\n        y = \"Number of Missing Values\"\n    ) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nWe can see that we have no missing values."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(readxl)\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\nlibrary(patchwork)\nlibrary(stringr)\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nlibrary(reshape2)\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nCode\nlibrary(sf)\n\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\n\nCode\nlibrary(jsonlite)\n\n\n\n\nCode\nlayoff_data &lt;- read.csv(\"./data/Clean/layoff_cleaned_2.csv\")\nlayoff_data &lt;- layoff_data |&gt; rename(Date = `Received.Date`)\nunemployment_data &lt;- read_excel(\"./data/Raw/Unemployment/UNRATE.xls\", skip = 10)\nunemployment_data &lt;- unemployment_data |&gt; rename(Date = observation_date)\n# head(layoff_data, 5)\n\n\n\n\nCode\nlayoff_data$Date &lt;- as.POSIXct(layoff_data$Date, format = \"%Y-%m-%d\")\nlayoff_data$effective_date_cleaned &lt;- as.POSIXct(layoff_data$effective_date_cleaned, format = \"%Y-%m-%d\")\n\n\n\n\nCode\ndji_stock_indices_data &lt;- read.csv(\"./data/Raw/Stock_Indices/DowJonesIndex.csv\")\nsnp_stock_indices_data &lt;- read.csv(\"./data/Raw/Stock_Indices/S&P500.csv\")\nnasdaq_stock_indices_data &lt;- read.csv(\"./data/Raw/Stock_Indices/NASDAQ.csv\")\n\ndji_stock_indices_data$Date &lt;- as.POSIXct(dji_stock_indices_data$Date, format = \"%m/%d/%y\")\nsnp_stock_indices_data$Date &lt;- as.POSIXct(snp_stock_indices_data$Date, format = \"%m/%d/%y\")\nnasdaq_stock_indices_data$Date &lt;- as.POSIXct(nasdaq_stock_indices_data$Date, format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Time series plot for layoff numbers\nlayoff_data_2015 &lt;- layoff_data %&gt;%\n    filter(Date &gt; as.Date(\"2015-01-01\"))\n\n# Extract month and year from Date\nagg_layoffs &lt;- layoff_data_2015 %&gt;%\n    group_by(month = format(Date, \"%Y-%m\")) %&gt;%\n    summarize(total_laid_off = sum(`Number.of.Workers`))\n\nagg_layoffs$month &lt;- as.Date(paste0(agg_layoffs$month, \"-01\"), format = \"%Y-%m-%d\")\n\nggplot(agg_layoffs, aes(x = month, y = total_laid_off)) +\n    geom_line() +\n    geom_point() +\n    labs(\n        title = \"Number.of.Workers Laid Off Each Month\",\n        x = \"Month\",\n        y = \"Total Laid Off\"\n    ) +\n    theme_minimal()\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCode\nplot_ly(layoff_data, x = ~Date, y = ~`Number.of.Workers`, type = \"scatter\", mode = \"lines\") %&gt;%\n    layout(\n        title = \"Time Series Plot for Number of Layoffs\",\n        xaxis = list(title = \"Data Laid Off\"),\n        yaxis = list(title = \"Number.of.Workers\")\n    )\n\n\n\n\n\n\nInferences:\n\nWe can see that number of layoffs between 2015 and 2023 peaked in March 2020 with around 23695 employees laid off and in July 2023, where around 20000 employees were laid off. The layoffs were high throughout 2020 and entered 2021. The next season of layoffs occured around April - July in 2023.\n\n\n\nCode\nurl &lt;- \"https://www2.census.gov/geo/tiger/GENZ2022/shp/cb_2022_us_state_20m.zip\"\ntemp &lt;- tempfile(fileext = \".zip\")\ndownload.file(url, temp, mode = \"wb\")\nunzip(temp, exdir = \"./data/Raw/Map\")\n\nstates &lt;- st_read(\"./data/Raw/Map/cb_2022_us_state_20m.shp\")\n\n\nReading layer `cb_2022_us_state_20m' from data source \n  `C:\\Users\\lenovo\\Documents\\Columbia\\Courses\\Fall 2023\\EDAV\\Assignments\\Project\\EDAV-finalproject-Fall2023\\data\\Raw\\Map\\cb_2022_us_state_20m.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 52 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1743 ymin: 17.91377 xmax: 179.7739 ymax: 71.35256\nGeodetic CRS:  NAD83\n\n\nCode\nlayoff_data_city &lt;- layoff_data_2015 %&gt;%\n    group_by(State) %&gt;%\n    summarize(total_laid_off_state = sum(`Number.of.Workers`))\n\ncity_data &lt;- merge(states, layoff_data_city, by.x = \"NAME\", by.y = \"State\", all.x = TRUE)\n\nggplot() +\n    geom_sf(data = city_data, aes(fill = total_laid_off_state), color = \"white\", size = 0.2) +\n    scale_fill_viridis_c(labels = scales::number_format(scale = 1e-3, suffix = \"K\")) +\n    theme_minimal() +\n    labs(title = \"Choropleth Map of Number.of.Workers Laid off in Each State\") +\n    coord_sf(xlim = c(-125, -66))\n\n\n\n\n\nInferences:\n\nWe can see that California has the highest number of layoffs so far with a very high difference compared to other states. This makes sense as it is the hub of major companies.\n\n\n\nCode\nlayoff_data_2015$Date &lt;- as.Date(layoff_data_2015$Date, format = \"%Y-%m-%d\")\nlayoff_data_2015$effective_date_cleaned &lt;- as.Date(layoff_data_2015$effective_date_cleaned, format = \"%Y-%m-%d\")\n\n# Calculate the difference in days between the two dates\nlayoff_data_2015$days_difference &lt;- as.numeric(difftime(layoff_data_2015$effective_date_cleaned, layoff_data_2015$Date, units = \"days\"))\n\n# Plot the histogram\nggplot(layoff_data_2015, aes(x = days_difference)) +\n    geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    labs(\n        title = \"Distribution of Days Between Date Received and Effective Date\",\n        x = \"Days Difference\",\n        y = \"Frequency\"\n    ) +\n    theme_minimal()\n\n\nWarning: Removed 2830 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nInferences:\n\nHere we examine how much time employees were given to find another job by finding the distribution of the difference of days when they received the notice and their last working date. We observe that generally they are given around 180 - 200 days. However in some case there are outliers where they are given 1800 or so days, this appears to be a flaw in the data which can be investigated.\n\n\n\nCode\nremove_outliers_z &lt;- function(data, column, threshold = 0.5) {\n    z_scores &lt;- scale(data[[column]])\n    data[abs(z_scores) &lt; threshold, , drop = FALSE]\n}\n\n# Remove outliers\ndata_no_outliers_z &lt;- remove_outliers_z(layoff_data_2015, \"days_difference\")\n\n\nggplot(data_no_outliers_z, aes(x = days_difference)) +\n    geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    labs(\n        title = \"Distribution of Days Between Date Received and Effective Date\",\n        x = \"Days Difference\",\n        y = \"Frequency\"\n    ) +\n    theme_minimal()\n\n\nWarning: Removed 2830 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\nCode\n# df_no_missing &lt;- layoff_data[complete.cases(layoff_data$Date), ]\n# df_no_missing[is.na(df_no_missing$`Date`),]\n#\n\n\n\n\nCode\n# json_data &lt;- toJSON(df_no_missing, pretty = TRUE)\n# writeLines(json_data, \"data/Clean/layoff_json.json\")\n\n\n\n\nCode\n# Convert Date to the last date of the month\nlayoff_data &lt;- layoff_data %&gt;%\n    mutate(\n        last_day_of_month = floor_date(Date, \"month\") +\n            days(days_in_month(Date) - 1)\n    )\n\n# Aggregate data at the monthly level\nmonthly_aggregated &lt;- layoff_data %&gt;%\n    group_by(last_day_of_month) %&gt;%\n    summarize(num_of_workers_agg = sum(`Number.of.Workers`, na.rm = TRUE))\n\n\n\n\nCode\n# filtered_layoff_data$`num_workers_scaled` &lt;- filtered_layoff_data$`Number of Workers` / filtered_layoff_data$`Number of Workers`[1]*100\n# snp_stock_indices_data$close_scaled &lt;- snp_stock_indices_data$Close / snp_stock_indices_data$Close[1] * 100\n# dji_stock_indices_data$close_scaled &lt;- dji_stock_indices_data$Close / dji_stock_indices_data$Close[1] * 100\n# nasdaq_stock_indices_data$close_scaled &lt;- nasdaq_stock_indices_data$Close / nasdaq_stock_indices_data$Close[1] * 100\n\n\n\n\nCode\nstart_date &lt;- as.POSIXct(\"2007-01-01\")\nend_date &lt;- as.POSIXct(\"2019-12-31\")\n\nlayoff_plot &lt;- ggplot() +\n    geom_line(\n        data = monthly_aggregated |&gt; filter(last_day_of_month &gt;= start_date, last_day_of_month &lt;= end_date),\n        aes(x = last_day_of_month, y = num_of_workers_agg)\n    ) +\n    labs(\n        y = \"Layoffs per month\",\n        x = \"\",\n        title = \"Study of Layoff and Market Performace between 2007-2023\",\n        subtitle = \"Layoff Trend\",\n    ) +\n    theme_bw() +\n    # theme(plot.title = element_text(face = \"bold\")) +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\nstock_indices_plot &lt;- ggplot() +\n    geom_line(\n        data = snp_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"S&P500\")\n    ) +\n    geom_line(\n        data = dji_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"Dow Jones Index\")\n    ) +\n    geom_line(\n        data = nasdaq_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"NASDAQ Composite\")\n    ) +\n    labs(\n        y = \"Market Value\",\n        x = \"Time\",\n        subtitle = \"Major US Stock Indices\",\n    ) +\n    theme_bw() +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\nlayoff_plot / stock_indices_plot\n\n\n\n\n\nInferences:\n\nHere we study the relation between layoff trends and market trends between 2007-2019. We see that around 2007-2009 there is a significant dip in Dow Jones Index, and a corresponding spike in the number of layoffs.\n\n\n\nCode\nstart_date &lt;- as.POSIXct(\"2018-01-01\")\nend_date &lt;- as.POSIXct(\"2023-12-31\")\n\n\nlayoff_plot &lt;- ggplot() +\n    geom_line(\n        data = monthly_aggregated |&gt; filter(last_day_of_month &gt;= start_date, last_day_of_month &lt;= end_date),\n        aes(x = last_day_of_month, y = num_of_workers_agg)\n    ) +\n    labs(\n        y = \"Layoffs per month\",\n        x = \"\",\n        title = \"Study of Layoff and Market Performace between 2018-2023\",\n        subtitle = \"Layoff Trend\",\n    ) +\n    theme_bw() +\n    # theme(plot.title = element_text(face = \"bold\")) +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\nstock_indices_plot &lt;- ggplot() +\n    geom_line(\n        data = snp_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"S&P500\")\n    ) +\n    geom_line(\n        data = dji_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"Dow Jones Index\")\n    ) +\n    geom_line(\n        data = nasdaq_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"NASDAQ Composite\")\n    ) +\n    labs(\n        y = \"Market Value\",\n        x = \"Time\",\n        subtitle = \"Major US Stock Indices\",\n    ) +\n    theme_bw() +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\nlayoff_plot / stock_indices_plot\n\n\n\n\n\nInferences:\n\nHere we study market and layoff trends between 2018-2023. The market went low around 2020 which was during covid and it also resulted in increasing layoffs. However, entering into 2021, we notice that the market recovered and consequently, we notice that the layoff rate also declined.\n\n\n\nCode\nstart_date &lt;- as.POSIXct(\"2021-01-01\")\nend_date &lt;- as.POSIXct(\"2023-12-31\")\n\n\nlayoff_plot &lt;- ggplot() +\n    geom_line(\n        data = monthly_aggregated |&gt; filter(last_day_of_month &gt;= start_date, last_day_of_month &lt;= end_date),\n        aes(x = last_day_of_month, y = num_of_workers_agg)\n    ) +\n    labs(\n        y = \"Layoffs per month\",\n        x = \"\",\n        title = \"Study of Layoff and Market Performace between 2021-2023\",\n        subtitle = \"Layoff Trend\",\n    ) +\n    theme_bw() +\n    # theme(plot.title = element_text(face = \"bold\")) +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\nstock_indices_plot &lt;- ggplot() +\n    geom_line(\n        data = snp_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"S&P500\")\n    ) +\n    geom_line(\n        data = dji_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"Dow Jones Index\")\n    ) +\n    geom_line(\n        data = nasdaq_stock_indices_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n        aes(x = Date, y = Close, color = \"NASDAQ Composite\")\n    ) +\n    labs(\n        y = \"Market Value\",\n        x = \"Time\",\n        subtitle = \"Major US Stock Indices\",\n    ) +\n    theme_bw() +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\nlayoff_plot / stock_indices_plot\n\n\n\n\n\nInferences:\n\nWe narrow down our observation to study market and layoff trends in the recent times between 2021-2023. The dips in the market align with the peaks in the layoff, signifying a negative correlation between market conditions and layoff rates.\n\n\n\nCode\nstart_date &lt;- as.POSIXct(\"2018-01-01\")\nend_date &lt;- as.POSIXct(\"2023-12-31\")\npk &lt;- ggplot(\n    data = monthly_aggregated |&gt; filter(last_day_of_month &gt;= start_date, last_day_of_month &lt;= end_date),\n    aes(x = last_day_of_month, y = num_of_workers_agg)\n) +\n    geom_line() +\n    labs(\n        x = \"\",\n        y = \"Layoffs per month\",\n        title = \"Study of Layoff and Unemployment Trend between 2018-2023\",\n        subtitle = \"Layoff Trend\"\n    ) +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\npd &lt;- ggplot(\n    data = unemployment_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n    aes(x = Date, y = UNRATE)\n) +\n    geom_line() +\n    labs(\n        x = \"Time\",\n        y = \"Unemployment Rate\",\n        subtitle = \"Unemployment Trend\"\n    ) +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n# pk\npk / pd\n\n\n\n\n\nInferences:\n\nWe analyze the relation between unemloyment rates and the layoffs per month and whether the data supports our hypothesis that increasing layoffs and increasing unemployment rate are correlated. Between 2018 - 2023, we can see that the unemployment rate increased greatly in 2020 and steadily declined during 2021. This can also be seen in the layoff per month graph where the layoffs peaked in 2020 and started to come down in 2021.\n\n\n\nCode\nstart_date &lt;- as.POSIXct(\"2021-01-01\")\nend_date &lt;- as.POSIXct(\"2023-12-31\")\npk &lt;- ggplot(\n    data = monthly_aggregated |&gt; filter(last_day_of_month &gt;= start_date, last_day_of_month &lt;= end_date),\n    aes(x = last_day_of_month, y = num_of_workers_agg)\n) +\n    geom_line() +\n    labs(\n        x = \"\",\n        y = \"Layoffs per month\",\n        title = \"Study of Layoff and Unemployment Trend between 2021-2023\",\n        subtitle = \"Layoff Trend\"\n    ) +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n\npd &lt;- ggplot(\n    data = unemployment_data |&gt; filter(Date &gt;= start_date, Date &lt;= end_date),\n    aes(x = Date, y = UNRATE)\n) +\n    geom_line() +\n    labs(\n        x = \"Time\",\n        y = \"Unemployment Rate\",\n        subtitle = \"Unemployment Trend\"\n    ) +\n    scale_y_continuous(labels = scales::number_format(scale = 1))\n# pk\npk / pd\n\n\n\n\n\nInferences:\n\nNarrowing down our analysis to recent trends between 2021-2023, we see that there is a slight inconsistency in the trends. Although the unemployment rates are decreasing in 2022 and 2023, there is a significant increase in number of layoffs in 2023.\n\n\n\nCode\nlayoff_data$year &lt;- as.numeric(format(layoff_data$Date, \"%Y\"))\n\nlayoff_reason_summary &lt;- layoff_data |&gt;\n    drop_na(layoff_type_cleaned) |&gt;\n    group_by(year, layoff_type_cleaned) |&gt;\n    summarize(total_layoffs = sum(`Number.of.Workers`, na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\n# options(repr.plot.width = 20, repr.plot.height = 12)\nstart_date &lt;- 2018\nend_date &lt;- 2023\n\n# Create a Cleveland dot plot\nggplot(\n    layoff_reason_summary |&gt; filter(year &gt;= start_date, year &lt;= end_date),\n    aes(y = reorder(layoff_type_cleaned, total_layoffs), x = total_layoffs)\n) +\n    geom_point() +\n    facet_wrap(~year, nrow = 1) +\n    labs(\n        y = \"Layoff Reason\",\n        x = \"# of Workers Laid off\",\n        title = \"Study of Layoff reasons over the recent years 2018 - 2023\"\n    ) +\n    theme_bw() +\n    scale_x_continuous(labels = scales::number_format(scale = 1))\n\n\n\n\n\nInferences:\n\nWith the above graph, we want to analyse the major reason for layoffs across different years from 2018-2023. Most of the layoffs in 2020 were temporary because businesses were expecting to recover with time, but in other years, the layoffs although much lower in number are of permanent nature.\n\n\n\nCode\n# each year's top 10 industries in terms of layoffs\nindustry_summary &lt;- layoff_data |&gt;\n    drop_na(industry_cleaned) |&gt;\n    group_by(year, industry_cleaned) |&gt;\n    summarize(total_layoffs = sum(`Number.of.Workers`, na.rm = TRUE)) |&gt;\n    ungroup() |&gt;\n    group_by(year) %&gt;%\n    arrange(desc(total_layoffs)) %&gt;%\n    slice_head(n = 10)\n\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\n# options(repr.plot.width = 20, repr.plot.height = 12)\nstart_date &lt;- 2020\nend_date &lt;- 2023\n\n# Create a Cleveland dot plot\nggplot(\n    industry_summary |&gt; filter(year &gt;= start_date, year &lt;= end_date),\n    aes(y = reorder(industry_cleaned, total_layoffs), x = total_layoffs)\n) +\n    geom_point() +\n    facet_wrap(~year, nrow = 1) +\n    labs(\n        y = \"Industry\",\n        x = \"# of Workers Laid off\",\n        title = \"Study of Layoff trend across industries over the recent years 2018 - 2023\"\n    ) +\n    theme_bw() +\n    scale_x_continuous(labels = scales::number_format(scale = 1))\n\n\n\n\n\nInferences:\n\nWe can see that most of the layoffs in 2020 occured in the Restaurant and Hotels industry. This was very much expected because this industry was among the worst affected industries due to the COVID lockdowns.\nManufacturing industry was among the top 10 industries in year 2021-2023 but not in 2020. This indicates that the manufacturing industry was not instantly affected during the COVID lockdowns but did suffer later.\nProfessional, Scientific, and Technical Services was not among the top 10 most affected industries from 2020-2022, but in 2023 it was affected. This is evident from the recent job market."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "In conclusion, our exploratory data analysis of layoff trends, unemployment data, and their correlation with major US stock indices reveals key insights. Layoffs peaked notably in March 2020 and July 2023, with California consistently leading in job displacements. Notice periods generally fall within 180-200 days, but anomalies, including extended notice periods, warrant further investigation.\nHistorical analysis indicates a negative correlation between market conditions and layoff rates, exemplified by the 2007-2009 economic downturn. In the recent context (2018-2023), the COVID-19-related market downturn in 2020 coincided with increased layoffs, followed by recovery in 2021. However, an intriguing inconsistency arises in 2023, where layoffs increase despite decreasing unemployment rates, signaling evolving employment dynamics.\nThe relationship between unemployment rates and layoffs aligns with expectations, with peaks in both metrics in 2020 and subsequent declines in 2021. Industry-specific analysis reveals shifts, such as the temporary nature of 2020 layoffs in the Restaurant and Hotels sector and delayed impacts on Manufacturing. The Professional, Scientific, and Technical Services sector sees increased layoffs in 2023, reflecting evolving job market dynamics.\nIn summary, our analysis underscores the dynamic interplay between economic conditions, layoffs, and industry-specific factors. Ongoing vigilance and investigation of anomalies are essential for refining the accuracy and reliability of our insights in this dynamic landscape."
  }
]