# Data

## Technical Description

The datasets we are using for our project are as follows:

1) Layoff Data: https://layoffdata.com/data/Raw/Stock_Indices/
  - Collected by: The federal WARN Act requires large employers to give advance notice of layoffs to state governments and workers. Though states publish this information, no entity collects these layoff notices across the many states.
  The WARN Database standardizes WARN Notices across the country into a single dataset and the only comprehensive database of worker layoffs.
  - Downloaded as an xlsx file from the website
  - Format of the data: excel spreadsheet with the following features:
    - State 
    - Company
    - City 
    - Number of Workers laid off
    - Received Date - date when the notice was received
    - Effective Date - date of leaving
    - Layoff/Closure Type - reason for layoff
    - Union - union to which workers belonged to if any
    - Region 
    - County
    - Industry - type of industry that the company belongs
    - Notes - further information on the reason for layoff
    - Frequency of updates: Monthly
  - Dimensions of the data: The data consists of 50218 rows and 13 columns.
  - Issues/problems with the data:
    There are two columns in the data namely: â€˜Layoff/Closure\n Type' and 'Temporary/Permanent' which have coinciding values     of the reason for layoff. We will pick one which is more explanatory of the layoff cause.
    
2) Unemployment Rate Data: https://fred.stlouisfed.org/series/UNRATE
  - Collected by Federal Reserve Economic Data from the U.S. Bureau of Labor Statistics
  - Download as an excel file available on the website
  - Format of the data : .xls file with the following features
    - Observation date: date on which the unemployment rate was taken
    - Rate of unemployment
  - Frequency of update: Monthly
  - Dimensions of the data: the data has 910 rows and 2 columns
  - Other relevant information: the data consists monthly employment rate from January 1984 - October 2023
  - Issues/problems with the data: there is no problem with the data, we will trim the data to accommodate recent years as we     do not need unemployment rates as old as 1984.
  
3) Stock market data: https://www.wsj.com/market-data/quotes/index/SPX/historical-prices
  - Collected by the Wall Street Journal
  - Downloaded as a csv file 
  - Format of the data: .csv file with the following features:
    - Date 
    - Open - opening prices for the day
    - High - highest price of the day
    - Low - lowest price of the day
  - Close - closing prices for the day
  - Frequency of the data: Daily
  - Dimensions of the data: The data has 4089 rows and 5 columns
  - Other relevant information: The data ranges from September 2007 to November 2023
  
Plan to import the data: We have downloaded the data from the sources mentioned above and will be using it in our R workspace for further analysis.


## Research Plan

Commencing our analysis, we will direct our attention to current layoff statistics over time, with the aim of understanding prevailing trends in the job market landscape. If current layoff data shows a notable increase, we can reasonably infer a potentially challenging job market scenario.

Simultaneously, we will conduct a comparative analysis with key US stock indices, particularly the S&P 500, to identify potential correlations with market performance. We anticipate a negative correlation, suggesting that a sluggish market performance may coincide with an uptick in current job terminations as organizations strive to optimize expenditures amidst reduced business activity.

Deepening our inquiry, our objective will be to identify patterns that could serve as indicators for current trajectories. Leveraging current layoff data, we will systematically identify sectors experiencing higher instances of workforce reduction. With a focus on the period from 2018 to 2023, encompassing the last five years, we acknowledge the challenge of comparing raw layoff numbers across industries due to variations in total employment. Therefore, our approach will involve an intra-industry comparison over time, aiming to distill trends that may identify the most impacted sectors. Conversely, industries with a relative absence of pronounced spikes in current layoffs may be considered more resilient in economic downturns.

Expanding our analysis, we will shift focus to a geographically nuanced examination of trends within the current US landscape. This aims to determine whether specific states or regions bear a disproportionate burden of current layoffs. Integrating current regional data with industry-specific metrics will provide insights into whether certain industries within particular states face more challenging economic circumstances.

Continuing our trajectory, we will examine the current temporal dynamics of layoffs, looking at the pace at which organizations are implementing workforce reductions. Analyzing the time organizations are affording employees to adapt to impending layoffs across diverse industries, we will also consider state-by-state variations to understand the influence of current regional legislative frameworks.

Examining the causative factors behind current layoffs available in data, we will undertake a meticulous analysis to identify predominant factors shaping current workforce reduction scenarios. This investigative phase, adopting a region-wise and state-wise lens, will aim to discern nuanced patterns that may emerge.

Finally, our analysis will integrate unemployment rate data to understand the post-layoff trajectories of affected individuals. This examination seeks to ascertain whether individuals subjected to current layoffs will transition swiftly into alternative employment or contribute to an uptick in current unemployment rates by remaining unemployed.

## Missing value analysis
We will now investigate our data to check for any missing values.

```{r}
library(dplyr)
library(tibble)
library(tidyr)
library(ggplot2)
library(forcats)
library(readxl)
```

### Layoff Dataset

```{r}
data <- read_excel("./data/Raw/Layoff/WARN Database 10-31-2023 [TO EDIT_ FILE-_MAKE A COPY].xlsx")

head(data, 5)
```

Let's check for the distribution of missing values across columns in our data.
```{r}
# Calculate the number of missing values in each column
missing_values <- sapply(data, function(x) sum(is.na(x)))

# Calculate the total number of records in the dataset
total_records <- nrow(data)

# Create a data frame for plotting
missing_data_df <- data.frame(Column = names(missing_values), Missing_Values = missing_values)

# Create a bar chart using ggplot2 with text labels on top and total records
ggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "black") +
    geom_text(aes(label = Missing_Values), vjust = -0.5, color = "black") +
    geom_text(aes(x = 0, y = total_records + 5, label = paste("Total Records:", total_records)),
        hjust = 0, vjust = 0, color = "black"
    ) +
    labs(
        title = "Number of Missing Values in Each Column",
        x = "Column",
        y = "Number of Missing Values"
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

We can make the following observations:

- We have Company and State information for all records.
- 'Union' and 'Temporary/Permanent' information is missing for almost all records, so we cannot include them in our analysis.
- We have around 612 records with no information of the 'Number of Workers' laid off, so these records cannot contribute to our analysis.
- Columns like 'Notes', and 'Region' have high fraction of missing values but they don't play a major role in our analysis. 
- The high fraction of missing values in 'Industry' coulmn, however, could limit our industry wise analysis. Since the critical parts of our analysis will depend on the last 5 years, we can check the missing ratio in that time interval to check if it's any better.

```{r}
data$`Received Date` <- as.POSIXct(data$`Received Date`, format = "%Y-%m-%d %Z")

# Filter for values after 2018
filtered_data <- data %>%
    filter(`Received Date` > as.POSIXct("2018-01-01"))


# Calculate the number of missing values in each column
missing_values <- sapply(filtered_data, function(x) sum(is.na(x)))

# Calculate the total number of records in the dataset
total_records <- nrow(filtered_data)

# Create a data frame for plotting
missing_data_df <- data.frame(Column = names(missing_values), Missing_Values = missing_values)

# Create a bar chart using ggplot2 with text labels on top and total records
ggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "black") +
    geom_text(aes(label = Missing_Values), vjust = -0.5, color = "black") +
    geom_text(aes(x = 0, y = total_records + 5, label = paste("Total Records:", total_records)),
        hjust = 0, vjust = 0, color = "black"
    ) +
    labs(
        title = "Number of Missing Values in Each Column (2018-2023)",
        x = "Column",
        y = "Number of Missing Values"
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

We do not see any good signs in this time interval since the fraction of missing values in the 'Industry' column is still high. Although, we see that we don't have any missing values in the 'Number of Workers' laid off.

Now, switching back to our original dataset, let's drop the 'Union', 'Temporary/Permanent', 'Notes', 'Region' columns before we deep dive into our missing values analyis as they might shadow the otherwise interesting patterns. We will also drop records with missing values in the 'Number of Workers' laid off.

```{r}
# Clean the data
data <- data |>
    select(-"Union", -"Temporary/Permanent", -"Notes", -"Region")

data <- data[!is.na(data[, "Number of Workers"]), ]
head(data)
```

Now, to identify patterns in missing values, we added a new feature to the redav package to address the chalenges of large datasets. We added the parameter 'threshold'. This controls the percentage of records covered by patterns to be displayed in the graph. For example, if we want to just display patterns which cover 90% of our data, we can pass a value of 0.9. 

Additionally, we also inclined the x-labels.

```{r}
plot_missing <- function(x, percent = TRUE, threshold = 1) {
    na_count_all <- data.frame(is.na(x)) %>%
        dplyr::group_by_all() %>%
        dplyr::count(name = "count", sort = TRUE) %>%
        dplyr::ungroup() %>%
        tibble::rownames_to_column("pattern")

    na_count_all <- na_count_all %>%
        dplyr::mutate(pattern = factor(.data$pattern, levels = nrow(na_count_all):1))

    # count the number of columns with missing values; will be used later to determine if there's a "none missing" pattern
    na_count_all <- na_count_all %>%
        dplyr::rowwise() %>%
        dplyr::mutate(num_missing_cols = sum(dplyr::c_across(where(is.logical))))

    # filter out based on threshold : NEWLY ADDED
    na_count_all <- na_count_all %>%
        arrange(desc(count)) %>%
        mutate(perc_pattern_cov = count / sum(na_count_all$count, na.rm = TRUE))
    na_count_all$cum_perc_pattern <- cumsum(na_count_all$perc_pattern_cov)
    na_count_all <- na_count_all |>
        filter(cum_perc_pattern <= threshold) |>
        select(-cum_perc_pattern, -perc_pattern_cov)

    # data frame for missing patterns bar chart
    na_count_by_pattern <- na_count_all[, c("pattern", "count", "num_missing_cols")]
    na_count_by_pattern$none_missing <- ifelse(na_count_by_pattern$num_missing_cols == 0, TRUE, FALSE)

    # data frame for missing by column bar chart
    na_count_by_column <- data.frame(is.na(x)) %>%
        colSums() %>%
        sort(decreasing = TRUE) %>%
        tibble::enframe(name = "var", value = "count")

    # tidy and sort na_count_all by column counts
    na_count_all_tidy <- na_count_all %>%
        tidyr::pivot_longer(where(is.logical), names_to = "variable") %>%
        dplyr::mutate(variable = factor(.data$variable, levels = na_count_by_column$var)) %>%
        dplyr::mutate(none_missing = ifelse(.data$num_missing_cols == 0, TRUE, FALSE))

    # main plot
    main_plot <- ggplot2::ggplot(na_count_all_tidy, ggplot2::aes(.data$variable, .data$pattern, fill = factor(.data$value), alpha = .data$none_missing)) +
        ggplot2::geom_tile(color = "white") +
        ggplot2::scale_fill_manual(values = c("grey70", "mediumpurple")) +
        ggplot2::scale_alpha_manual(values = c(.7, 1)) +
        ggplot2::ylab("missing pattern") +
        ggplot2::guides(fill = "none", alpha = "none") +
        ggplot2::theme_classic(12) +
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) #NEWLY ADDED

    # check for "none missing" pattern
    none_missing_pattern <- na_count_by_pattern %>%
        dplyr::filter(.data$none_missing) %>%
        dplyr::pull(.data$pattern)

    if (length(none_missing_pattern) > 0) {
        main_plot <- main_plot +
            ggplot2::annotate("text",
                x = (ncol(na_count_all) - 2) / 2,
                y = nrow(na_count_all) + 1 - as.numeric(as.character(none_missing_pattern)),
                label = "complete cases"
            )
    }

    # margin plots

    denom <- ifelse(percent, nrow(x) / 100, 1)

    missing_by_column_plot <- ggplot2::ggplot(na_count_by_column, ggplot2::aes(forcats::fct_inorder(.data$var), .data$count / denom)) +
        ggplot2::geom_col(fill = "cornflowerblue", alpha = .7) +
        ggplot2::scale_y_continuous(expand = c(0, 0), n.breaks = 3) +
        ggplot2::xlab("") +
        ggplot2::ylab(ifelse(percent, "% rows \n missing:", "num rows \n missing:")) +
        ggplot2::theme_linedraw(12) +
        ggplot2::theme(
            panel.grid.major.x = ggplot2::element_blank(),
            panel.grid.minor.x = ggplot2::element_blank()
        ) +
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) # NEWLY ADDED

    missing_by_pattern_plot <-
        ggplot2::ggplot(na_count_by_pattern, ggplot2::aes(.data$pattern, .data$count / denom, alpha = .data$none_missing)) +
        ggplot2::geom_col(fill = "cornflowerblue") +
        ggplot2::coord_flip() +
        ggplot2::scale_y_continuous(expand = c(0, 0), n.breaks = 3) +
        ggplot2::scale_alpha_manual(values = c(.7, 1)) +
        ggplot2::xlab("") +
        ggplot2::ylab(ifelse(percent, "% rows", "row count")) +
        ggplot2::guides(alpha = "none") +
        ggplot2::theme_linedraw(12) +
        ggplot2::theme(
            panel.grid.major.y = ggplot2::element_blank(),
            panel.grid.minor.y = ggplot2::element_blank()
        )

    if (percent) {
        missing_by_column_plot <- missing_by_column_plot +
            ggplot2::scale_y_continuous(
                expand = c(0, 0), n.breaks = 5,
                limits = c(0, 100)
            )
        missing_by_pattern_plot <- missing_by_pattern_plot +
            ggplot2::scale_y_continuous(
                expand = c(0, 0), n.breaks = 5,
                limits = c(0, 100)
            )
    }

    missing_by_column_plot + patchwork::plot_spacer() +
        main_plot + missing_by_pattern_plot +
        patchwork::plot_layout(widths = c(4, 1), heights = c(1, 4))
}
```

```{r}
plot_missing(data, threshold = 0.99, percent = FALSE)
```

We can make the following observations:

- As expected, 'Industry' column accounts for most missing values but in most cases it is the only column absent.
- 'Complete Cases' form the second most observed pattern which is a good sign.


### Unemployment Dataset

```{r}
data <- read_excel("./data/Raw/Unemployment/UNRATE.xls", skip=10)

head(data, 5)
```

Let's check for the distribution of missing values across columns in our data.
```{r}
# Calculate the number of missing values in each column
missing_values <- sapply(data, function(x) sum(is.na(x)))

# Calculate the total number of records in the dataset
total_records <- nrow(data)

# Create a data frame for plotting
missing_data_df <- data.frame(Column = names(missing_values), Missing_Values = missing_values)

# Create a bar chart using ggplot2 with text labels on top and total records
ggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "black") +
    geom_text(aes(label = Missing_Values), vjust = -0.5, color = "black") +
    geom_text(aes(x = 0, y = total_records + 5, label = paste("Total Records:", total_records)),
        hjust = 0, vjust = 0, color = "black"
    ) +
    labs(
        title = "Number of Missing Values in Each Column",
        x = "Column",
        y = "Number of Missing Values"
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
We can see that we have no missing values.

### Stock Market Dataset (S&P500)

```{r}
data <- read.csv("./data/Raw/Stock_Indices/S&P500.csv")

head(data, 5)
```

Let's check for the distribution of missing values across columns in our data.
```{r}
# Calculate the number of missing values in each column
missing_values <- sapply(data, function(x) sum(is.na(x)))

# Calculate the total number of records in the dataset
total_records <- nrow(data)

# Create a data frame for plotting
missing_data_df <- data.frame(Column = names(missing_values), Missing_Values = missing_values)

# Create a bar chart using ggplot2 with text labels on top and total records
ggplot(missing_data_df, aes(x = Column, y = Missing_Values)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "black") +
    geom_text(aes(label = Missing_Values), vjust = -0.5, color = "black") +
    geom_text(aes(x = 0, y = total_records + 5, label = paste("Total Records:", total_records)),
        hjust = 0, vjust = 0, color = "black"
    ) +
    labs(
        title = "Number of Missing Values in Each Column",
        x = "Column",
        y = "Number of Missing Values"
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
We can see that we have no missing values.